/*
 * Copyright (C) 2001-2012 Quantum ESPRESSO group
 * Copyright (C) 2010-2012 Irish Centre for High-End Computing (ICHEC)
 *
 * This file is distributed under the terms of the
 * GNU General Public License. See the file `License'
 * in the root directory of the present distribution,
 * or http://www.gnu.org/copyleft/gpl.txt .
 *
 */


*** Last update: September 18, 2012 (Filippo Spiga) ***


Authors:
- Filippo Spiga, spiga.filippo@gmail.com
- Ivan Girotto, igirotto@ictp.it

Any suggestion or improvement is welcome.
              

**************************  R E A D   T H I S  **************************

This version is fully compatible with QE-5.0.1

Rebuild date: September 18, 2012
Rebuild by: Filippo Spiga
Rebuild version: 1

**************************************************************************

# (strongly suggested) Requirements               

- Intel compiler (versions above 11), PGI compiler (tested only 12.2), 
  GNU (version above 4.1)
- NVIDIA cards (compute capability >= 1.3)
- CUDA 4.x (4.0 and 4.1 well tested, 4.2 only partially), CUDA 5.x-pre
- MKL (version >=10.x) or ACML (>= 4.4.x, >= 5.x)
- python >= 2.4.x

# How to compile 


(...after checkout the entire QE repository from SVN or 
   untar QE-GPU.tar.gz in your working directory...)   
  
$ cd GPU
$ ./configure --enable-parallel --enable-openmp \
  --enable-cuda --with-gpu-arch=20 \
  --with-cuda-dir=/ichec/packages/cuda/4.0/cuda --enable-magma
$ cd ..
$ make -f Makefile.gpu pw-gpu

'pw-gpu.x' executable will be placed under bin/


GPU-oriented options of the configure:

--enable-cuda           : turn on the GPU support

--with-cuda-dir=<path>  : specify the path where CUDA is installed (mandatory)

--with-gpu-arch=<arch>  : it can be '13' for cc1.3, '20' for cc20 or '30' for 
                          new KEPLER GPU (cc30). By default is '20'

--enable-phigemm        : enable BLAS Level 3 GPU acceleration using phiGEMM 
				           (default: yes)
				  
--enable-magma		    : enable GPU LAPACK acceleration based on MAGMA 1.1.0
                          Good for serial, less for parallel run (default: no)
                                             
--enable-profiling      : turn on the phiGEMM call-per-call profile
                          (default: no)

--enable-debug-cuda     : turn on verbose debug messages on the stdout

--enable-openacc	    : enable OpenACC (PGI only)


IMPORTANT NOTE (1): run 'configure' in the root directory will produce a 
                    make.sys for CPU-ONLY execution. Do this in a first 
                    instance if you are going to build both CPU and CPU+GPU
                    versions.

IMPORTANT NOTE (2): remember to have CUDA nvcc and CUDA libs properly set in 
                    the environment

IMPORTANT NOTE (3): sometimes is not possible to link "-lcuda" because the file 
                    'libcuda.so' is not in the system. Usually this file is present 
                    if the card and the nVidia driver (not the SDK, the driver) are 
                    installed. In such case, the configure add automatically the 
                    flag "-D__CUDA_GET_MEM_HACK" and the part of the code 
                    responsible to detect the GPU memory is skipped. In this case, 
                    you MUST edit the file "include/cuda_env.h" and manually change 
                    the value of "__GPU_MEM_AMOUNT_HACK__". See the notes in the 
                    file for more info.


# Suggested configurations/compiler flags (by Filippo)

*** PLEASE KEEP ALL THESE NOTES AS ADVICES, NOT RULES WRITTEN IN THE STONE ***

Be careful to do not...
- mix GPU compute capability: chose ALWAYS the right one ("13" for 1.3 cards or
  "20" for 2.x card or "30" for GTX 6xx )
- specify the correct PATH of CUDA (where CUDA is installed). Usually HPC 
  systems have the env variable $CUDA_HOME or $CUDA_ROOT
- run too small systems using the GPU. The GPu implementation has been 
  designed to accelerate heavy calculations. If you have a small system with 
  few atoms (something that entirely converge in few minutes) please use the
  standard MPI+OpenMP implementation
- in serial mode, GPU cannot handle too big calculations. if you workstation 
  has lot of RAM (> 12 GByte) remember that, whatever GPU you have, its memory
  is very limited. If you cannot run the GPU PWscf due to memory limits try to
  run (with GPU) but on a parallel machine

Best software configuration is...
- CUDA 4.0 (because it has been tested a lot on multiple systems)
- Intel compiler (it exists a free version under academic license)
- Intel MKL (whatever version above 10)
- Open MPI (IMHO, the best MPI library for process placement and affinity)

Best hardware configuration is...
- NVIDIA cards with compute capability 2.x:
   - mainstream HPC card, C20xx or 20xx
   - customer game cards, GTX 4xx or GTX 6xx (I never did tests on GTX 5xx) 
- for a system with less than 50 atoms, 1.5GByte are usually enough 
- good amount of memory on the card is 3 GByte
- more memory there is on the card, better it is

For cards with compute capability 1.3...
- MAGMA does not perform as expected, it generates not precise numbers and the 
  iterative diagonalization fails. Suggestion: --disable-magma (but you might 
  want to check if your input still works with MAGMA)
- CUDA 4.0 is mandatory. Other new versions of CUDA might not work (there is 
  some tuning to do, probably soon there will be full support)

For system with GPU GCC/GFORTRAN compilers...
- be sure that GNU version is above 4.1
- GCC 4.5 appears to be compatible only with CUDA 4.2 and above
- if you want to link BLAS/LAPACK Intel MKL libraries to QE add 
  the following (--enable-openmp case)
    BLAS_LIBS="-L${MKL_LIB} -lmkl_intel_lp64 -lmkl_gnu_thread -lmkl_core" LAPACK_LIBS=" "
  or the following (--disable-openmp case)
    BLAS_LIBS="-L${MKL_LIB} -lmkl_intel_lp64 -lmkl_gnu_sequential -lmkl_core" LAPACK_LIBS=" "
  at the end of the ./configure
- "--enable-profiling" does not work (I am working on it...)

For system with PGI Compilers...
- versions above 12.2 experience a problem at run-time due to the IOTK library.
  Be sure to compile by adding the flag -D__IOTK_WORKAROUND1
    make MANUAL_DFLAGS="-D__IOTK_WORKAROUND1" pw
- do not trust the BLAS/LAPACK (AMCL) implementation provided by PGI. if you 
  are using Intel processors on your system download the Intel MKl library and 
  link link it to QE by adding the following (--enable-openmp case)
    BLAS_LIBS="-L${MKL_LIB} -lmkl_intel_lp64 -lmkl_pgi_thread -lmkl_core" LAPACK_LIBS=" "
  or the following (--disable-openmp case)
    BLAS_LIBS="-L${MKL_LIB} -lmkl_intel_lp64 -lmkl_pgi_sequential -lmkl_core" LAPACK_LIBS=" "
  at the end of the ./configure

For CRAY XK6 systems...
- be sure to compile by adding the flag -D__IOTK_WORKAROUND1 if PGI compiler is
  used (make MANUAL_DFLAGS="-D__IOTK_WORKAROUND1" pw)
- CRAY compiler is not supported
- phiGEMM profiling does not work properly at the moment (no idea why)
- XK6 systems usually have Intel compiler and MKL. If it is the case, use 
  this compiler
- experimental OpenACC stuff are in the code but please... try it if you know
  what are you doing
- be sure to unload the module "atp  totalview-support xt-totalview hss-llm"
- if you want to use PGI compiler, then use ACML 5.0.0 (provided by CRAY in the 
  environment but not as default) and manually add ACML in this way
    ./configure --enable-openmp --enable-cuda --with-gpu-arch=20 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} \
     --enable-magma --disable-profiling --enable-phigemm --enable-parallel ARCH=crayxt \
     BLAS_LIBS="-L/opt/acml/5.0.0/pgi64_mp/lib -lacml_mp" LAPACK_LIBS="  "
- ALWAYS specify "ARCH=crayxt" in the configure, for example
    ./configure --enable-openmp --enable-cuda --with-gpu-arch=20 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} \ 
     --enable-magma --disable-profiling --enable-phigemm --enable-parallel ARCH=crayxt
- on some CRAY XK6 systems, if Intel compilers are used you also need to add
  "F90=ifort" on the configure
    ./configure --enable-openmp --enable-cuda --with-gpu-arch=20 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} \ 
     --enable-magma --disable-profiling --enable-phigemm --enable-parallel ARCH=crayxt F90=ifort

Few best practices:
- NEVER oversubscribe too much the GPU! on HPC system equipped with NVIDIA 
  M20xx you might place 2 MPI process on the same card, no more. The best is 
  1 MPI process per card. So for example (PBS script and Open MPI):
  
#PBS -l walltime=04:00:00
#PBS -l select=4:mpiprocs=4:ncpus=12:ngpus=2
#PBS -o job.out
#PBS -e job.err
#PBS -q parallel

export OMP_NUM_THREADS=3
export MKL_NUM_THREADS=${OMP_NUM_THREADS}

export SPLIT_COEFF=9
export PHI_DGEMM_SPLIT=0.${SPLIT_COEFF}
export PHI_ZGEMM_SPLIT=0.${SPLIT_COEFF}
export PHI_CGEMM_SPLIT=0.${SPLIT_COEFF}
export PHI_SGEMM_SPLIT=0.${SPLIT_COEFF}

mpirun -v -np 16 -npernode 4  -bysocket -bind-to-socket -display-map -report-bindings -tag-output ./pw-gpu.x -input <your_input>
  
  this is a good way to run PWscf with GPU in parallel on 4 nodes (each node 
  has 2 six-core Intel processor and 2 NVIDIA GPU) by placing 4 MPI process
  per node (-npernode 4) distributed and binded in a round-robin fashion to
  the sockets (-bysocket -bind-to-socket). Each GPU is used simultaneously by 
  two MPI processes (because mpiprocs=4 and ngpus=2)
- PHI_{D,Z,C,S}GEMM_SPLIT should be different from system to system BUT for 
  long execution it does not really matter because the phiGEMM library is able
  to self-adjust this parameter by "analyzing" the performance of previous
  GEMM calls
- the best configuration include MPI+OpenMP+GPU. Remember to set 
  OMP_NUM_THREADS and MKL_NUM_THREADS accordingly to your system
- Quantum ESPRESSO has been written in many years by people that KNOW how to 
  code and optimize a HPC code. So, adding the GPU should bring a speed-up that
  might vary between a bit more than 2-times (lower bound for parallel case) to 
  7-times (higher bound for serial case). Do not EXPECT an acceleration with 
  two digits!
- Performance is very sensitive of input type and (physical) system under 
  study. It might happen that some inputs perform bad. The developers cannot 
  face this kind of problem if they cannot test THAT specific input. So if you 
  are disappointed about the performance or the support... WRITE TO US!


# How to run

Nothing has changed. If the support to the GPU is enables you MUST remember to 
specify few additional environmental variables required by the phiGEMM library 
used by Quantum EPSRESSO.

There are 4 recognized variables, one for each *GEMM routine implemented. This 
factor, a value strictly between 0 and 1, tells to the library how much of the 
calculation has to be performed on the CPU and how much on the GPU. In detail, 
if PHI_DGEMM_SPLIT is 0.9 it means that the 95% of the computation will be 
performed by the GPU, the 5% by the CPU.

Here an example:

export PHI_SGEMM_SPLIT=0.95
export PHI_CGEMM_SPLIT=0.875
export PHI_DGEMM_SPLIT=0.9
export PHI_ZGEMM_SPLIT=0.925

IMPORTANT NOTE: suggested values are between 0.875 and 0.95

IMPORTANT NOTE: some system are equipped with different cards. For example, you 
                may have 2 Tesla Cxxxx and another card, that you use for the 
                video output. The video card may have not enough CUDA 
                capabilities for computation (below 1.3) or different technical
                specs. There is a way to hide a specific GPU card, using the
                environmental variable "CUDA_VISIBLE_DEVICES".
                
IMPORTANT NOTE: the library auto-tune the split factor at runtime. You just have to declare a
                initial good guess. If you are not sure about the guess value leave the default.         


# Trouble-makers. inconsistencies, etc: 

- if you are using Intel MPI, please add to DFLAGS "-DMPICH_SKIP_MPICXX" to 
   ignore MPI C++ bindings.


# In case of CUDA/GPU code errors...
        
It may happen that some ported routines fail. If this happen you can easily
decide to run the code skipping that specific "GPU" part that produce some 
issues. To do that it is possible to compile the code with few additional
flags that exclude the GPU code related to ADDUSDENS, NEWD or VLOC_PSI. 

These flags are: 
-D__DISABLE_CUDA_ADDUSDENS, -D__DISABLE_CUDA_VLOCPSI, 
-D__DISABLE_CUDA_NEWD, -D__PHIGEMM_DISABLE_SPECIALK

Without editing the make.sys, you can run make in this way:
$ make MANUAL_DFLAGS="-D__DISABLE_CUDA_ADDUSDENS" pw

You can also specify more of these flags together.


# Acknowledgments

- This work has been supported by the PRACE 1st Implementation Phase project, WP 7.5e
"Programming Techniques for High-Performance Applications - Accelerator". The 
Irish Centre for High-End Computing (ICHEC) has been directly responsible to 
extend the  Quantum ESPRESSO suite in order to support GPU calculation using 
the NVIDIA CUDA environment.
