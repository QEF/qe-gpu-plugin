/*
 * Copyright (C) 2001-2013 Quantum ESPRESSO group
 *
 * This file is distributed under the terms of the
 * GNU General Public License. See the file `License'
 * in the root directory of the present distribution,
 * or http://www.gnu.org/copyleft/gpl.txt .
 *
 */

*** Last update: November 29, 2013 (Filippo Spiga) ***

*** Requirements               

- Intel compiler (versions above 11) or PGI compiler (tested up to 12.8)
- GNU GCC/GFORTRAN (version above 4.1 but lower than 4.6)
- NVIDIA cards (compute capability >= 2.0, best 3.5)
- CUDA 5.x
- MKL (version >=10.x) or ACML (>= 4.4.x, >= 5.x)
- python >= 2.4.x
- Quantum ESPRESSO svn

*** How to compile 
 
$ cd GPU
$ ./configure --disable-parallel --enable-openmp \
  --enable-cuda --with-gpu-arch=35 \
  --with-cuda-dir=/opt/cuda5.0 --with-magma --with-phigemm
$ cd ..
$ make -f Makefile.gpu all-gpu

"*-gpu.x" executables will be placed under bin/ (like "pw-gpu.x")

For additional options for QE-GPU see ./configure --help

In order to assess the best flag combination on a specific system we suggest
to exploit all these 4 configurations by running one or two steps before run 
the entire (long) simulation:
1) --with-phigemm --with-magma
2) --with-phigemm --with-magma --enable-fast-cuda
3) --with-phigemm --with-magma --enable-pinned-mem
4) --with-phigemm --with-magma --enable-pinned-mem --enable-fast-cuda

In detail:
1) is the default, it has to work. If it does not please contact us.
2) turn on the phiGEMM "SPECIAL-K" feature and allow the GPU to use FMA 
   operation at HW level. Due to this option, results might be slightly 
   different than a reference computed using the CPU. This is perfectly
   normal if numbers stay within a safe tolerance.
3) enabling pinned memory will speed-up lot of heavy GEMM operations and
   will help MAGMA to perform overlap between CPU and GPU. However if the
   application requests too much pinned memory it might be counterproductive
4) if pinned memory works well and there is a significant gain in performance
   then it is possible to combine --enable-pinned-mem and --enable-fast-cuda
   to get the maximum from the GPU


   * PLEASE KEEP ALL THESE NOTES AS ADVICES, NOT RULES WRITTEN IN THE STONE *

- specify ALWAYS the right GPU compute capability.
- specify the correct PATH of CUDA (where CUDA is installed). Usually HPC 
  systems have the env variable $CUDA_HOME or $CUDA_ROOT
- run too small systems using the GPU. The GPU implementation has been 
  designed to accelerate heavy calculations. If you have a small system with 
  few atoms (something that entirely converge in few minutes) please use the
  standard MPI+OpenMP implementation
- in serial mode, GPU cannot handle too big calculations. if you workstation 
  has lot of RAM (> 12 GByte) remember that, whatever GPU you have, its memory
  is very limited. If you cannot run the GPU PWscf due to memory limits try to
  run (with GPU) but on a parallel machine

*** How to run

Nothing has changed. If the support to the GPU is enables you MUST remember to 
specify few additional environmental variables required by the phiGEMM library 
used by Quantum EPSRESSO.

There are 4 recognized variables, one for each *GEMM routine implemented. This 
factor, a value strictly between 0 and 1, tells to the library how much of the 
calculation has to be performed on the CPU and how much on the GPU. In detail, 
if PHI_DGEMM_SPLIT is 0.9 it means that the 95% of the computation will be 
performed by the GPU, the 5% by the CPU.

Here an example:


export PHI_DGEMM_SPLIT=0.975
export PHI_ZGEMM_SPLIT=0.975

IMPORTANT NOTE (1): suggested values are between 0.925 and 0.975

IMPORTANT NOTE (2): some system are equipped with different cards. For 
                    example, you may have 2 Tesla Cxxxx and another card, that 
                    you use for the video output. The video card may have not 
                    enough CUDA capabilities for computation (below 1.3) or 
                    different technical specs. There is a way to hide a 
                    specific GPU card, using the environmental variable 
                    "CUDA_VISIBLE_DEVICES".
                
IMPORTANT NOTE (3): the library auto-tune the split factor at runtime. You just 
                    have to declare a initial good guess. If you are not sure 
                    about the guess value leave the default.      

                
*** 5. Best Practices

- NEVER oversubscribe too much the GPU! On HPC system equipped with NVIDIA 
  M20xx you might place 2 MPI process on the same card, no more. The best is 
  1 MPI process per card. So for example (PBS script and Open MPI):
  
#PBS -l walltime=04:00:00
#PBS -l select=4:mpiprocs=4:ncpus=12:ngpus=2
#PBS -o job.out
#PBS -e job.err
#PBS -q parallel

export OMP_NUM_THREADS=3
export MKL_NUM_THREADS=${OMP_NUM_THREADS}

export SPLIT_COEFF=9
export PHI_DGEMM_SPLIT=0.${SPLIT_COEFF}
export PHI_ZGEMM_SPLIT=0.${SPLIT_COEFF}
export PHI_CGEMM_SPLIT=0.${SPLIT_COEFF}
export PHI_SGEMM_SPLIT=0.${SPLIT_COEFF}

mpirun -v -np 16 -npernode 4  -bysocket -bind-to-socket -display-map -report-bindings -tag-output ./pw-gpu.x -input <your_input>
  
  this is a good way to run PWscf with GPU in parallel on 4 nodes (each node 
  has 2 six-core Intel processor and 2 NVIDIA GPU) by placing 4 MPI process
  per node (-npernode 4) distributed and binded in a round-robin fashion to
  the sockets (-bysocket -bind-to-socket). Each GPU is used simultaneously by 
  two MPI processes (because mpiprocs=4 and ngpus=2)
- PHI_{D,Z,C,S}GEMM_SPLIT should be different from system to system BUT for 
  long execution it does not really matter because the phiGEMM library is able
  to self-adjust this parameter by "analyzing" the performance of previous
  GEMM calls
- the best configuration include MPI + OpenMP + GPU. Remember to set 
  OMP_NUM_THREADS and MKL_NUM_THREADS accordingly to your system
- Quantum ESPRESSO has been written in many years by people that KNOW how to 
  code and optimize a HPC code. So, adding the GPU should bring a speed-up that
  might vary between a bit more than 2-times (lower bound for parallel case) to 
  7-times (higher bound for serial case). Do not EXPECT an acceleration with 
  two digits!
  
... and remember ...

- Performance is very sensitive of input type and (physical) system under 
  study. It might happen that some inputs perform bad. The developers cannot 
  face this kind of problem if they cannot test THAT specific input. So if you 
  are disappointed about the performance or the support... WRITE TO US!
   


*** Trouble-makers. inconsistencies, etc: 

- if you are using Intel MPI, please add to DFLAGS "-DMPICH_SKIP_MPICXX" to 
  ignore MPI C++ bindings.


*** In case of CUDA/GPU code errors...
        
It may happen that some ported routines fail. If this happen you can easily
decide to run the code skipping that specific "GPU" part that produce some 
issues. To do that it is possible to compile the code with few additional
flags that exclude the GPU code related to ADDUSDENS, NEWD or VLOC_PSI. 

These flags are: 
-D__DISABLE_CUDA_ADDUSDENS 
-D__DISABLE_CUDA_VLOCPSI 
-D__DISABLE_CUDA_NEWD

Without editing the make.sys, you can run make in this way:
$ make -f Makefile.gpu MANUAL_DFLAGS="-D__DISABLE_CUDA_ADDUSDENS" pw-gpu

You can also specify more of these flags together.


*** 8. Profiling capabilities (CPU-only)

It is possible to use the QE-GPU package to extend the standard QE code and
allow call-by-call profiling of GEMM operations also for CPU-only execution.

$ cd GPU
$ ./configure --enable-parallel --enable-openmp \
  --disable-cuda --enable-phigemm --enable-profiling
$ cd ..
$ make -f Makefile.cpu all

All the executables (like "pw.x") will appear under "bin/". You run your 
calculation normally like the standard QE package. At the end of the execution 
several files will be created in the same working directory. Example:

-rw-r--r--  1 fspiga fislab  77572 2013-01-02 11:02 phigemm.profile.3.csv
-rw-r--r--  1 fspiga fislab  66270 2013-01-02 11:02 phigemm.profile.2.csv
-rw-r--r--  1 fspiga fislab  69142 2013-01-02 11:02 phigemm.profile.1.csv
-rw-r--r--  1 fspiga fislab 105941 2013-01-02 11:02 phigemm.profile.0.csv

The number indicates the MPI rank (within MPI_COMM_WORLD). If the number is 
missing then the execution was serial. The files contains information in CSV
about each GEMM operation performed. Calls are listed as they temporarly 
occour during the execution.


*** 9. References

[1] P. Giannozzi et al., "QUANTUM ESPRESSO: a modular and open-source software 
    project for quantum simulations of materials", J. Phys. 
    Condens. Matter 21 395502 (2009)
[2] URL: http://www.quantum-espresso.org 
[3] F. Spiga and I. Girotto, "phiGEMM: a CPU-GPU library for porting 
    Quantum ESPRESSO on hybrid systems.", 20th Euromicro International 
    Conference on Parallel, Distributed and Network-Based Processing 
    (PDP), 2012
[4] Z. Xianyi, W. Qian, Z. Yunquan, "Model-driven Level 3 BLAS Performance 
    Optimization on Loongson 3A Processor", 2012 IEEE 18th International 
    Conference on Parallel and Distributed Systems (ICPADS), 17-19 Dec. 2012.
[5] URL: http://xianyi.github.com/OpenBLAS/


*** 10. Acknowledgments

[2011-2012] The first phase of this work has been supported by the PRACE 1st 
Implementation Phase project, WP 7.5e "Programming Techniques for High 
Performance Applications - Accelerator". The Irish Centre for High-End 
Computing (ICHEC) has been directly responsible to extend the Quantum ESPRESSO
suite in order to support GPU calculation using the NVIDIA CUDA environment.
- Filippo Spiga (ICHEC, Ireland)
- Ivan Girotto  (ICHEC, Ireland)

[2012-2013]
- Filippo Spiga (University of Cambridge, Quantum ESPRESSO Foundation)
- Ivan Girotto  (ICTP)
